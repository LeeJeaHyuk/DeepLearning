{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT로 텍스트 분류\n",
    "https://www.tensorflow.org/text/tutorials/classify_text_with_bert?hl=ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT 소개\n",
    "BERT 및 기타 변압기 인코더 아키텍처는 NLP의 다양한 작업 (자연 언어 처리)에 격렬하게 성공했다. 그들은 딥 러닝 모델에 사용하기에 적합한 자연어의 벡터 공간 표현을 계산합니다. BERT 모델 제품군은 Transformer 인코더 아키텍처를 사용하여 이전 및 이후의 모든 토큰의 전체 컨텍스트에서 입력 텍스트의 각 토큰을 처리하므로 이름: Bidirectional Encoder Representations from Transformers.\n",
    "\n",
    "BERT 모델은 일반적으로 대량의 텍스트에 대해 사전 학습된 다음 특정 작업에 맞게 미세 조정됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install -q -U tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deepBlue\\deepblue\\compitition\\dacon\\daconenv\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\deepBlue\\deepblue\\compitition\\dacon\\daconenv\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.11.0 and strictly below 2.14.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.get_logger().setLevel('ERROR')은 TensorFlow의 로깅 레벨을 설정하는 코드입니다. \n",
    "- 로깅은 프로그램 실행 중에 발생하는 메시지를 기록하고 출력하는 작업을 말합니다. \n",
    "- setLevel('ERROR')는 로깅 레벨을 'ERROR'로 설정하는 것을 의미합니다. \n",
    "- 이는 로그 메시지 중에서 오류에 관련된 메시지만 표시하고, 경고 및 정보 메시지는 표시하지 않도록 설정하는 것을 의미합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84125825/84125825 [==============================] - 133s 2us/step\n"
     ]
    }
   ],
   "source": [
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "\n",
    "dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# remove unused folders to make it easier to load the data\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tf.keras.utils.get_file 함수는 파일을 다운로드하고 로컬 파일 시스템에 저장하는 유틸리티 함수입니다.\n",
    "-   tf.keras.utils.get_file(\n",
    "        fname,\n",
    "        origin,\n",
    "        untar=False,\n",
    "        md5_hash=None,\n",
    "        file_hash=None,\n",
    "        cache_dir=None,\n",
    "        cache_subdir='datasets',\n",
    "        extract=False,\n",
    "        archive_format='auto',\n",
    "        cache_subdir_lock=True\n",
    "    )\n",
    "    - fname: 다운로드할 파일의 이름을 지정합니다.\n",
    "    - origin: 파일의 원격 경로 또는 URL을 지정합니다. url\n",
    "    - untar (기본값: False): True로 설정하면 tar 파일을 해제합니다. (압축 파일 해제)\n",
    "    - cache_dir: 다운로드한 파일의 저장 위치를 지정합니다.\n",
    "    - cache_subdir (기본값: 'datasets'): cache_dir에서 파일을 저장할 하위 디렉토리를 지정합니다.\n",
    "\n",
    "- dataset_dir: dataset의 디렉토리 경로를 가져온 후, 'aclImdb'라는 폴더명을 추가하여 데이터셋의 최종 저장 위치를 지정합니다.\n",
    "- train_dir: dataset_dir에서 'train' 폴더의 경로를 생성합니다.\n",
    "- remove_dir: train_dir에서 'unsup' 폴더의 경로를 생성합니다.\n",
    "- shutil.rmtree(remove_dir): remove_dir에 해당하는 폴더를 완전히 삭제합니다.\n",
    "    - unsup 폴더는 IMDB 데이터셋의 일부인 unlabeled 데이터를 포함하는 폴더입니다. 이 폴더에는 레이블이 지정되지 않은 영화 리뷰 데이터가 포함되어 있습니다.\n",
    "    - 일반적으로 IMDB 데이터셋은 감정 분석을 위해 사용되며, 각 리뷰에는 긍정적인 감정 또는 부정적인 감정이 할당됩니다. 하지만 unsup 폴더의 데이터는 레이블이 없기 때문에 이 데이터는 감정 분석 모델의 비지도 학습에 사용되거나, 다른 목적으로 활용될 수 있습니다.\n",
    "    - 이 작업은 해당 데이터를 사용하지 않는 경우, 데이터를 삭제하여 불필요한 처리나 메모리 사용을 줄일 수 있도록 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "class_names = raw_train_ds.class_names\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AUTOTUNE = tf.data.AUTOTUNE\n",
    "    - TensorFlow의 AUTOTUNE 기능을 사용합니다. 이는 TensorFlow가 자동으로 최적화된 값을 선택하도록 합니다.\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "- raw_train_ds: text_dataset_from_directory \n",
    "    - 함수를 사용하여 훈련 데이터를 로드합니다. 지정된 디렉토리('aclImdb/train')에서 텍스트 파일을 읽어와서 배치 형태로 데이터셋을 생성합니다. validation_split 인자를 사용하여 훈련 데이터 중 일부를 검증 데이터로 분할하고, subset 인자를 사용하여 훈련 데이터로 선택된 부분을 지정합니다.\n",
    "    - aclImdb/train : 데이터가 저장된 디렉토리 경로\n",
    "    - validation_split : 데이터를 훈련 및 검증 데이터로 분할하는 비율을 지정합니다.\n",
    "    - subset='training' : 'training' 또는 'validation' 중 하나를 선택하여 훈련 데이터 또는 검증 데이터로 지정합니다.\n",
    "\n",
    "- class_names: 데이터셋에서 사용되는 클래스(레이블)의 이름을 저장합니다.\n",
    "\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "- train_ds: 훈련 데이터셋을 캐시하고(데이터를 메모리나 디스크에 저장하여 나중에 재사용할 수 있도록 합니다), 학습 전에 필요한 데이터를 미리 로드하여 성능을 향상시킵니다. prefetch 함수를 사용하여 데이터를 비동기적으로 로드합니다.(데이터를 배치 단위로 로드하는 동안 모델의 학습이나 추론 작업을 동시에 수행할 수 있도록 해줍니다.)\n",
    "    - buffer_size 매개변수는 데이터를 미리 로드해놓을 버퍼의 크기를 지정합니다. \n",
    "       - buffer_size=1000으로 설정하면, 다음 배치를 로드하기 전에 1000개의 데이터를 미리 로드해놓습니다.\n",
    "       - 이렇게 미리 로드해놓은 데이터는 모델이 학습 중에 사용되므로 데이터 로딩에 따른 대기 시간이 줄어들어 전체 학습 과정이 더욱 효율적으로 이루어질 수 있습니다.\n",
    "       - buffer_size=AUTOTUNE을 설정하면 TensorFlow가 최적의 버퍼 크기를 자동으로 결정하게 됩니다.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daconenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
